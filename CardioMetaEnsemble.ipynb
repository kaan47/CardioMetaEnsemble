{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc7795-6420-4bc2-929b-63d23eebfef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Integrative Ensemble and Meta-Learning Models for Cardiovascular Risk Prediction\n",
    "# Authors: Kaan Kara, MSc; Oykum Esra Yigit, Assoc. Prof. Dr.; Tuba Gunel, Prof. Dr.*\n",
    "# Date: 2025-09-05\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f0e90-e817-4881-b995-9cd94ec0cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Cardiovascular Risk Prediction - Data Loading\n",
    "# =============================================\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for clean output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------------------\n",
    "# Load dataset\n",
    "# ----------------------------\n",
    "file_path = \"heart_statlog_cleveland_hungary_final.csv\"  # Path to your CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Replace spaces in column names with underscores for easier access\n",
    "data.columns = [col.replace(\" \", \"_\") for col in data.columns]\n",
    "\n",
    "# ----------------------------\n",
    "# Display dataset info\n",
    "# ----------------------------\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "print(\"\\nDataset information:\")\n",
    "print(data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ab192b-8992-4fe7-8007-83ada1a8e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Feature Identification and Scaling\n",
    "# =============================================\n",
    "\n",
    "# Define categorical and continuous features\n",
    "categorical_features = ['sex', 'chest_pain_type', 'fasting_blood_sugar',\n",
    "                        'resting_ecg', 'exercise_angina', 'ST_slope', 'target']\n",
    "\n",
    "continuous_features = ['age', 'resting_bp_s', 'cholesterol', 'max_heart_rate', 'oldpeak']\n",
    "\n",
    "# Display feature types\n",
    "print(\"Categorical features:\", categorical_features)\n",
    "print(\"Continuous features:\", continuous_features)\n",
    "\n",
    "# ----------------------------\n",
    "# Feature Scaling\n",
    "# ----------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Columns to be scaled\n",
    "numerical_cols = ['age', 'resting_bp_s', 'cholesterol', 'max_heart_rate', 'oldpeak']\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform only numerical columns\n",
    "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "print(\"\\nScaled dataset preview:\")\n",
    "print(data.head())\n",
    "\n",
    "# ----------------------------\n",
    "# Train-Test Split\n",
    "# ----------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data: 80% training, 20% testing\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display dataset sizes\n",
    "print(f\"\\nTotal data points: {len(data)}\")\n",
    "print(f\"Training data points: {len(train_data)}\")\n",
    "print(f\"Testing data points: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c86ef8-b276-4972-8a13-a6681d928b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# AdaBoost Model Training and Evaluation\n",
    "# =============================================\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import KFold, GridSearchCV, learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Feature and target separation\n",
    "# ----------------------------\n",
    "X_train = train_data.drop(columns=['target'])\n",
    "y_train = train_data['target']\n",
    "X_test = test_data.drop(columns=['target'])\n",
    "y_test = test_data['target']\n",
    "\n",
    "# ----------------------------\n",
    "# Initial AdaBoost model\n",
    "# ----------------------------\n",
    "adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred = adaboost_model.predict(X_test)\n",
    "\n",
    "# Model accuracy\n",
    "accuracy_adaboost = accuracy_score(y_test, y_pred)\n",
    "print(f\"Initial AdaBoost Model Accuracy: {accuracy_adaboost:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# K-Fold Cross-Validation\n",
    "# ----------------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "adaboost_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    adaboost_model.fit(X_fold_train, y_fold_train)\n",
    "    y_fold_pred = adaboost_model.predict(X_fold_val)\n",
    "    adaboost_scores.append(accuracy_score(y_fold_val, y_fold_pred))\n",
    "\n",
    "print(f\"K-Fold Mean Accuracy (Initial Model): {np.mean(adaboost_scores):.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Hyperparameter Tuning with GridSearchCV\n",
    "# ----------------------------\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=AdaBoostClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and score\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validated Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate best model on test set\n",
    "# ----------------------------\n",
    "best_adaboost_model = grid_search.best_estimator_\n",
    "y_pred = best_adaboost_model.predict(X_test)\n",
    "\n",
    "accuracy_best = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy of Best AdaBoost Model: {accuracy_best:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# K-Fold Cross-Validation for Best Model\n",
    "# ----------------------------\n",
    "adaboost_scores = []\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    best_adaboost_model.fit(X_fold_train, y_fold_train)\n",
    "    y_fold_pred = best_adaboost_model.predict(X_fold_val)\n",
    "    adaboost_scores.append(accuracy_score(y_fold_val, y_fold_pred))\n",
    "\n",
    "print(f\"K-Fold Mean Accuracy (Best Model): {np.mean(adaboost_scores):.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Model Performance Metrics\n",
    "# ----------------------------\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC Curve (if predict_proba is available)\n",
    "if hasattr(best_adaboost_model, \"predict_proba\"):\n",
    "    fpr, tpr, _ = roc_curve(y_test, best_adaboost_model.predict_proba(X_test)[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "else:\n",
    "    fpr, tpr, roc_auc = [None] * 3\n",
    "\n",
    "# Learning Curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_adaboost_model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "# ----------------------------\n",
    "# Plot ROC Curve and Learning Curve\n",
    "# ----------------------------\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ROC Curve\n",
    "if fpr is not None:\n",
    "    ax[0].plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    ax[0].plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    ax[0].set_xlim([0.0, 1.0])\n",
    "    ax[0].set_ylim([0.0, 1.05])\n",
    "    ax[0].set_xlabel('False Positive Rate')\n",
    "    ax[0].set_ylabel('True Positive Rate')\n",
    "    ax[0].set_title('ROC Curve')\n",
    "    ax[0].legend(loc='lower right')\n",
    "else:\n",
    "    ax[0].text(0.5, 0.5, 'ROC Curve Not Available', horizontalalignment='center',\n",
    "               verticalalignment='center', fontsize=12)\n",
    "    ax[0].set_title('ROC Curve')\n",
    "\n",
    "# Learning Curve\n",
    "ax[1].plot(train_sizes, train_mean, 'o-', color='red', label='Training Score')\n",
    "ax[1].plot(train_sizes, test_mean, 'o-', color='blue', label='Cross-Validation Score')\n",
    "ax[1].set_xlabel('Training Examples')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Learning Curve')\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead0394-a279-4220-90d3-9e793002b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Gradient Boosting Machine (GBM) Model Training and Evaluation\n",
    "# =============================================\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score, learning_curve\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Initial GBM model\n",
    "# ----------------------------\n",
    "gbm_model = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, random_state=42)\n",
    "gbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred_gbm = gbm_model.predict(X_test)\n",
    "\n",
    "# Model accuracy\n",
    "accuracy_gbm = accuracy_score(y_test, y_pred_gbm)\n",
    "print(f\"Initial GBM Model Accuracy: {accuracy_gbm:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# K-Fold Cross-Validation for GBM\n",
    "# ----------------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gbm_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    gbm_model.fit(X_fold_train, y_fold_train)\n",
    "    y_fold_pred_gbm = gbm_model.predict(X_fold_val)\n",
    "    gbm_scores.append(accuracy_score(y_fold_val, y_fold_pred_gbm))\n",
    "\n",
    "print(f\"K-Fold Mean Accuracy (Initial GBM): {np.mean(gbm_scores):.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Hyperparameter Tuning with GridSearchCV\n",
    "# ----------------------------\n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "gbm_kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "gbm_grid_search = GridSearchCV(\n",
    "    estimator=GradientBoostingClassifier(random_state=42),\n",
    "    param_grid=gbm_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=gbm_kf,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gbm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and cross-validated accuracy\n",
    "print(f\"Best Hyperparameters: {gbm_grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validated Accuracy: {gbm_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate best GBM model on test set\n",
    "# ----------------------------\n",
    "best_gbm_model = gbm_grid_search.best_estimator_\n",
    "y_pred_best_gbm = best_gbm_model.predict(X_test)\n",
    "accuracy_best_gbm = accuracy_score(y_test, y_pred_best_gbm)\n",
    "print(f\"Test Accuracy of Best GBM Model: {accuracy_best_gbm:.4f}\")\n",
    "\n",
    "# K-Fold Cross-Validation for Best GBM Model\n",
    "kfold_scores = cross_val_score(best_gbm_model, X_train, y_train, cv=gbm_kf, scoring='accuracy', n_jobs=-1)\n",
    "print(\"K-Fold Cross-Validation Accuracy Scores:\", kfold_scores)\n",
    "print(f\"Mean K-Fold Accuracy: {kfold_scores.mean():.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Model Performance Metrics\n",
    "# ----------------------------\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_best_gbm))\n",
    "\n",
    "# ROC Curve (check if predict_proba is available)\n",
    "if hasattr(best_gbm_model, \"predict_proba\"):\n",
    "    fpr_gbm, tpr_gbm, _ = roc_curve(y_test, best_gbm_model.predict_proba(X_test)[:, 1])\n",
    "    roc_auc_gbm = auc(fpr_gbm, tpr_gbm)\n",
    "else:\n",
    "    fpr_gbm, tpr_gbm, roc_auc_gbm = [None] * 3\n",
    "\n",
    "# Learning Curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_gbm_model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "# ----------------------------\n",
    "# Plot ROC Curve and Learning Curve\n",
    "# ----------------------------\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ROC Curve\n",
    "if fpr_gbm is not None:\n",
    "    ax[0].plot(fpr_gbm, tpr_gbm, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc_gbm:.2f})')\n",
    "    ax[0].plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    ax[0].set_xlim([0.0, 1.0])\n",
    "    ax[0].set_ylim([0.0, 1.05])\n",
    "    ax[0].set_xlabel('False Positive Rate')\n",
    "    ax[0].set_ylabel('True Positive Rate')\n",
    "    ax[0].set_title('ROC Curve')\n",
    "    ax[0].legend(loc='lower right')\n",
    "else:\n",
    "    ax[0].text(0.5, 0.5, 'ROC Curve Not Available', horizontalalignment='center',\n",
    "               verticalalignment='center', fontsize=12)\n",
    "    ax[0].set_title('ROC Curve')\n",
    "\n",
    "# Learning Curve\n",
    "ax[1].plot(train_sizes, train_mean, 'o-', color='red', label='Training Score')\n",
    "ax[1].plot(train_sizes, test_mean, 'o-', color='blue', label='Cross-Validation Score')\n",
    "ax[1].set_xlabel('Training Examples')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Learning Curve')\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2cdbc0-63a3-4392-832c-3d2a700c2b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# XGBoost Model Training and Evaluation\n",
    "# =============================================\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score, learning_curve\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Initial XGBoost model\n",
    "# ----------------------------\n",
    "xgb_model = XGBClassifier(n_estimators=50, learning_rate=0.1, random_state=42, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Model accuracy\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"Initial XGBoost Model Accuracy: {accuracy_xgb:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# K-Fold Cross-Validation for XGBoost\n",
    "# ----------------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "xgb_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    xgb_model.fit(X_fold_train, y_fold_train)\n",
    "    y_fold_pred_xgb = xgb_model.predict(X_fold_val)\n",
    "    xgb_scores.append(accuracy_score(y_fold_val, y_fold_pred_xgb))\n",
    "\n",
    "print(f\"K-Fold Mean Accuracy (Initial XGBoost): {np.mean(xgb_scores):.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Hyperparameter Tuning with GridSearchCV\n",
    "# ----------------------------\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and accuracy\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validated Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate best XGBoost model on test set\n",
    "# ----------------------------\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "y_pred_best_xgb = best_xgb_model.predict(X_test)\n",
    "accuracy_best_xgb = accuracy_score(y_test, y_pred_best_xgb)\n",
    "print(f\"Test Accuracy of Best XGBoost Model: {accuracy_best_xgb:.4f}\")\n",
    "\n",
    "# K-Fold Cross-Validation for Best XGBoost Model\n",
    "kfold_scores_xgb = cross_val_score(best_xgb_model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "print(\"K-Fold Cross-Validation Accuracy Scores:\", kfold_scores_xgb)\n",
    "print(f\"Mean K-Fold Accuracy: {kfold_scores_xgb.mean():.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Model Performance Metrics\n",
    "# ----------------------------\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_best_xgb))\n",
    "\n",
    "# ROC Curve (check if predict_proba is available)\n",
    "if hasattr(best_xgb_model, \"predict_proba\"):\n",
    "    fpr_xgb, tpr_xgb, _ = roc_curve(y_test, best_xgb_model.predict_proba(X_test)[:, 1])\n",
    "    roc_auc_xgb = auc(fpr_xgb, tpr_xgb)\n",
    "else:\n",
    "    fpr_xgb, tpr_xgb, roc_auc_xgb = [None] * 3\n",
    "\n",
    "# Learning Curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_xgb_model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "# ----------------------------\n",
    "# Plot ROC Curve and Learning Curve\n",
    "# ----------------------------\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ROC Curve\n",
    "if fpr_xgb is not None:\n",
    "    ax[0].plot(fpr_xgb, tpr_xgb, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc_xgb:.2f})')\n",
    "    ax[0].plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    ax[0].set_xlim([0.0, 1.0])\n",
    "    ax[0].set_ylim([0.0, 1.05])\n",
    "    ax[0].set_xlabel('False Positive Rate')\n",
    "    ax[0].set_ylabel('True Positive Rate')\n",
    "    ax[0].set_title('ROC Curve')\n",
    "    ax[0].legend(loc='lower right')\n",
    "else:\n",
    "    ax[0].text(0.5, 0.5, 'ROC Curve Not Available', horizontalalignment='center',\n",
    "               verticalalignment='center', fontsize=12)\n",
    "    ax[0].set_title('ROC Curve')\n",
    "\n",
    "# Learning Curve\n",
    "ax[1].plot(train_sizes, train_mean, 'o-', color='red', label='Training Score')\n",
    "ax[1].plot(train_sizes, test_mean, 'o-', color='blue', label='Cross-Validation Score')\n",
    "ax[1].set_xlabel('Training Examples')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Learning Curve')\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb9eccd-dfee-4010-80c5-bb0b0487188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# LightGBM Model Training and Evaluation\n",
    "# =============================================\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV, learning_curve\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Initial LightGBM model\n",
    "# ----------------------------\n",
    "lgbm_model = LGBMClassifier(n_estimators=50, learning_rate=0.1, random_state=42, verbose=-1)\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred_lgbm = lgbm_model.predict(X_test)\n",
    "\n",
    "# Model accuracy\n",
    "accuracy_lgbm = accuracy_score(y_test, y_pred_lgbm)\n",
    "print(f\"Initial LightGBM Model Accuracy: {accuracy_lgbm:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# K-Fold Cross-Validation\n",
    "# ----------------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "lgbm_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    lgbm_model.fit(X_fold_train, y_fold_train)\n",
    "    y_fold_pred_lgbm = lgbm_model.predict(X_fold_val)\n",
    "    lgbm_scores.append(accuracy_score(y_fold_val, y_fold_pred_lgbm))\n",
    "\n",
    "print(f\"K-Fold Mean Accuracy (Initial LightGBM): {np.mean(lgbm_scores):.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Hyperparameter Tuning with GridSearchCV\n",
    "# ----------------------------\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "lgbm_grid_search = GridSearchCV(\n",
    "    LGBMClassifier(random_state=42, verbose=-1),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "lgbm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Hyperparameters: {lgbm_grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validated Accuracy: {lgbm_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Train Best LightGBM Model\n",
    "# ----------------------------\n",
    "best_lgbm_model = LGBMClassifier(**lgbm_grid_search.best_params_, random_state=42)\n",
    "best_lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_best_lgbm = best_lgbm_model.predict(X_test)\n",
    "accuracy_best_lgbm = accuracy_score(y_test, y_pred_best_lgbm)\n",
    "print(f\"Test Accuracy of Best LightGBM Model: {accuracy_best_lgbm:.4f}\")\n",
    "\n",
    "# K-Fold Cross-Validation for Best Model\n",
    "best_lgbm_scores = []\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    best_lgbm_model.fit(X_fold_train, y_fold_train)\n",
    "    y_fold_pred_best_lgbm = best_lgbm_model.predict(X_fold_val)\n",
    "    best_lgbm_scores.append(accuracy_score(y_fold_val, y_fold_pred_best_lgbm))\n",
    "\n",
    "print(f\"K-Fold Mean Accuracy (Best LightGBM): {np.mean(best_lgbm_scores):.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# ROC Curve and Learning Curve\n",
    "# ----------------------------\n",
    "# ROC probabilities\n",
    "y_probs_lgbm = best_lgbm_model.predict_proba(X_test)[:, 1]\n",
    "fpr_lgbm, tpr_lgbm, _ = roc_curve(y_test, y_probs_lgbm)\n",
    "roc_auc_lgbm = auc(fpr_lgbm, tpr_lgbm)\n",
    "\n",
    "# Learning Curve\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_lgbm_model,\n",
    "    X_train, y_train,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    n_jobs=-1\n",
    ")\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ROC Curve\n",
    "axes[0].plot(fpr_lgbm, tpr_lgbm, color='blue', lw=2, label=f'AUC = {roc_auc_lgbm:.3f}')\n",
    "axes[0].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "axes[0].set_title('ROC Curve after Hyperparameter Tuning (LightGBM)')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].legend(loc='lower right')\n",
    "\n",
    "# Learning Curve\n",
    "axes[1].plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n",
    "axes[1].plot(train_sizes, val_mean, 'o-', color='red', label='Validation Score')\n",
    "axes[1].set_title('Learning Curve after Hyperparameter Tuning (LightGBM)')\n",
    "axes[1].set_xlabel('Number of Training Samples')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f679f87-30ea-44b6-a297-252cfaa622f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Stochastic Gradient Boosting (SGB) Model\n",
    "# =============================================\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV, learning_curve\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Initial SGB model\n",
    "# ----------------------------\n",
    "sgb_model = GradientBoostingClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "sgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred_sgb = sgb_model.predict(X_test)\n",
    "\n",
    "# Model accuracy\n",
    "accuracy_sgb = accuracy_score(y_test, y_pred_sgb)\n",
    "print(f\"Stochastic Gradient Boosting Model Accuracy: {accuracy_sgb:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# K-Fold Cross-Validation\n",
    "# ----------------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "sgb_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Train a temporary SGB model for each fold\n",
    "    sgb_temp = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, subsample=0.8, random_state=42)\n",
    "    sgb_temp.fit(X_fold_train, y_fold_train)\n",
    "    \n",
    "    y_fold_pred_sgb = sgb_temp.predict(X_fold_val)\n",
    "    sgb_scores.append(accuracy_score(y_fold_val, y_fold_pred_sgb))\n",
    "\n",
    "kfold_mean_sgb = np.mean(sgb_scores)\n",
    "print(f\"K-Fold Mean Accuracy (Initial SGB): {kfold_mean_sgb:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Hyperparameter Tuning\n",
    "# ----------------------------\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "sgb_model = GradientBoostingClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    sgb_model,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Train Best SGB Model\n",
    "# ----------------------------\n",
    "best_sgb_model = GradientBoostingClassifier(**best_params, random_state=42)\n",
    "best_sgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_best_sgb = best_sgb_model.predict(X_test)\n",
    "best_accuracy_sgb = accuracy_score(y_test, y_pred_best_sgb)\n",
    "print(f\"Test Accuracy of Best SGB Model: {best_accuracy_sgb:.4f}\")\n",
    "\n",
    "# K-Fold Cross-Validation for optimized model\n",
    "optimized_sgb_scores = []\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    sgb_temp = GradientBoostingClassifier(**best_params, random_state=42)\n",
    "    sgb_temp.fit(X_fold_train, y_fold_train)\n",
    "    \n",
    "    y_fold_pred_sgb = sgb_temp.predict(X_fold_val)\n",
    "    optimized_sgb_scores.append(accuracy_score(y_fold_val, y_fold_pred_sgb))\n",
    "\n",
    "kfold_mean_best_sgb = np.mean(optimized_sgb_scores)\n",
    "print(f\"K-Fold Mean Accuracy (Best SGB): {kfold_mean_best_sgb:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Classification Report, ROC and Learning Curve\n",
    "# ----------------------------\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_sgb))\n",
    "\n",
    "# ROC Curve\n",
    "if hasattr(best_sgb_model, \"predict_proba\"):\n",
    "    fpr, tpr, _ = roc_curve(y_test, best_sgb_model.predict_proba(X_test)[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "else:\n",
    "    fpr, tpr, roc_auc = [None]*3\n",
    "\n",
    "# Learning Curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    sgb_model,\n",
    "    X_train, y_train,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ROC Curve\n",
    "if fpr is not None:\n",
    "    ax[0].plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    ax[0].plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    ax[0].set_xlim([0.0, 1.0])\n",
    "    ax[0].set_ylim([0.0, 1.05])\n",
    "    ax[0].set_xlabel('False Positive Rate')\n",
    "    ax[0].set_ylabel('True Positive Rate')\n",
    "    ax[0].set_title('ROC Curve (SGB)')\n",
    "    ax[0].legend(loc='lower right')\n",
    "else:\n",
    "    ax[0].text(0.5, 0.5, 'ROC Curve Not Available', horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
    "    ax[0].set_title('ROC Curve')\n",
    "\n",
    "# Learning Curve\n",
    "ax[1].plot(train_sizes, train_mean, 'o-', color='red', label='Training Score')\n",
    "ax[1].plot(train_sizes, test_mean, 'o-', color='blue', label='Cross-Validation Score')\n",
    "ax[1].set_xlabel('Training Examples')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Learning Curve (SGB)')\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab77a3-390a-4925-8f0b-c6f3c6d5b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Random Forest (RF) Model\n",
    "# =============================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV, learning_curve\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Initial RF Model\n",
    "# ----------------------------\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Model accuracy\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Model Accuracy: {accuracy_rf:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# K-Fold Cross-Validation\n",
    "# ----------------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    rf_model.fit(X_fold_train, y_fold_train)\n",
    "    y_fold_pred_rf = rf_model.predict(X_fold_val)\n",
    "    rf_scores.append(accuracy_score(y_fold_val, y_fold_pred_rf))\n",
    "\n",
    "print(f\"K-Fold Mean Accuracy (Initial RF): {np.mean(rf_scores):.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Hyperparameter Tuning\n",
    "# ----------------------------\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search_rf.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Train Best RF Model\n",
    "# ----------------------------\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_best_rf = best_rf_model.predict(X_test)\n",
    "accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)\n",
    "print(f\"Test Accuracy of Best RF Model: {accuracy_best_rf:.4f}\")\n",
    "\n",
    "# K-Fold CV for optimized RF\n",
    "best_rf_scores = []\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    best_rf_model.fit(X_fold_train, y_fold_train)\n",
    "    y_fold_pred_best_rf = best_rf_model.predict(X_fold_val)\n",
    "    best_rf_scores.append(accuracy_score(y_fold_val, y_fold_pred_best_rf))\n",
    "\n",
    "print(f\"K-Fold Mean Accuracy (Best RF): {np.mean(best_rf_scores):.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Classification Report, ROC, Learning Curve\n",
    "# ----------------------------\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# ROC Curve\n",
    "if hasattr(rf_model, \"predict_proba\"):\n",
    "    fpr, tpr, _ = roc_curve(y_test, rf_model.predict_proba(X_test)[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "else:\n",
    "    fpr, tpr, roc_auc = [None]*3\n",
    "\n",
    "# Learning Curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    rf_model,\n",
    "    X_train, y_train,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ROC Curve\n",
    "if fpr is not None:\n",
    "    ax[0].plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    ax[0].plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    ax[0].set_xlim([0.0, 1.0])\n",
    "    ax[0].set_ylim([0.0, 1.05])\n",
    "    ax[0].set_xlabel('False Positive Rate')\n",
    "    ax[0].set_ylabel('True Positive Rate')\n",
    "    ax[0].set_title('ROC Curve (RF)')\n",
    "    ax[0].legend(loc='lower right')\n",
    "else:\n",
    "    ax[0].text(0.5, 0.5, 'ROC Curve Not Available', horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
    "    ax[0].set_title('ROC Curve')\n",
    "\n",
    "# Learning Curve\n",
    "ax[1].plot(train_sizes, train_mean, 'o-', color='red', label='Training Score')\n",
    "ax[1].plot(train_sizes, test_mean, 'o-', color='blue', label='Cross-Validation Score')\n",
    "ax[1].set_xlabel('Training Examples')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Learning Curve (RF)')\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d9885-3f15-437d-b953-c188c3336fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV, learning_curve\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# Base Random Forest for AdaBoost\n",
    "# ----------------------------\n",
    "rf_base = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf_boost = AdaBoostClassifier(estimator=rf_base, n_estimators=50, random_state=42)\n",
    "\n",
    "# Train initial model\n",
    "rf_boost.fit(X_train, y_train)\n",
    "\n",
    "# Test predictions\n",
    "y_pred_rf_boost = rf_boost.predict(X_test)\n",
    "accuracy_rf_boost = accuracy_score(y_test, y_pred_rf_boost)\n",
    "print(f\"Random Forest Boosting Model Accuracy: {accuracy_rf_boost:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_rf_boost))\n",
    "\n",
    "# ----------------------------\n",
    "# K-Fold Cross Validation\n",
    "# ----------------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf_boost_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    rf_boost.fit(X_fold_train, y_fold_train)\n",
    "    y_fold_pred = rf_boost.predict(X_fold_val)\n",
    "    rf_boost_scores.append(accuracy_score(y_fold_val, y_fold_pred))\n",
    "\n",
    "print(f\"K-Fold Mean Accuracy (RF + AdaBoost): {np.mean(rf_boost_scores):.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Hyperparameter Tuning\n",
    "# ----------------------------\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [50, 100, 150],\n",
    "    'estimator__max_depth': [3, 5, 10],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf_boost, param_grid, cv=kf, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(f\"Best K-Fold Accuracy: {best_score:.4f}\")\n",
    "\n",
    "# Test performance of best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# ROC Curve & Learning Curve\n",
    "# ----------------------------\n",
    "# ROC\n",
    "y_probs = best_model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Learning curve\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_model,\n",
    "    X_train, y_train,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    n_jobs=-1\n",
    ")\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ROC\n",
    "axes[0].plot(fpr, tpr, color='blue', lw=2, label=f'AUC = {roc_auc:.3f}')\n",
    "axes[0].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "axes[0].set_title('ROC Curve (RF + AdaBoost)')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].legend(loc='lower right')\n",
    "\n",
    "# Learning Curve\n",
    "axes[1].plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n",
    "axes[1].plot(train_sizes, val_mean, 'o-', color='red', label='Validation Score')\n",
    "axes[1].set_title('Learning Curve (RF + AdaBoost)')\n",
    "axes[1].set_xlabel('Training Examples')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c9e0bf-292a-4a80-8999-fd257c45e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV, learning_curve\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# Bagged Logistic Regression\n",
    "# ----------------------------\n",
    "bagged_lr = BaggingClassifier(\n",
    "    estimator=LogisticRegression(max_iter=5000, solver=\"saga\"),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "bagged_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Test seti tahmini\n",
    "y_pred_bagged_lr = bagged_lr.predict(X_test_scaled)\n",
    "accuracy_bagged_lr = accuracy_score(y_test, y_pred_bagged_lr)\n",
    "print(f\"Bagged Logistic Regression Accuracy: {accuracy_bagged_lr:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_bagged_lr))\n",
    "\n",
    "# ----------------------------\n",
    "# K-Fold Cross Validation\n",
    "# ----------------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "bagged_lr_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Ölçekleme\n",
    "    X_fold_train_scaled = scaler.transform(X_fold_train)\n",
    "    X_fold_val_scaled = scaler.transform(X_fold_val)\n",
    "    \n",
    "    bagged_lr.fit(X_fold_train_scaled, y_fold_train)\n",
    "    y_fold_pred = bagged_lr.predict(X_fold_val_scaled)\n",
    "    bagged_lr_scores.append(accuracy_score(y_fold_val, y_fold_pred))\n",
    "\n",
    "print(f\"K-Fold Mean Accuracy (Bagged LR): {np.mean(bagged_lr_scores):.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Hyperparameter Tuning\n",
    "# ----------------------------\n",
    "param_grid = {\n",
    "    \"estimator__C\": [0.01, 0.1, 1, 10],\n",
    "    \"n_estimators\": [50, 100, 200]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=BaggingClassifier(\n",
    "        estimator=LogisticRegression(max_iter=5000, solver=\"saga\"), random_state=42\n",
    "    ),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_bagged_lr = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(f\"Best K-Fold Accuracy: {best_score:.4f}\")\n",
    "\n",
    "# Test set performance\n",
    "y_pred_best = best_bagged_lr.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# ROC Curve & Learning Curve\n",
    "# ----------------------------\n",
    "y_probs = best_bagged_lr.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_bagged_lr,\n",
    "    X_train_scaled, y_train,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ROC\n",
    "axes[0].plot(fpr, tpr, color='blue', lw=2, label=f'AUC = {roc_auc:.3f}')\n",
    "axes[0].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "axes[0].set_title('ROC Curve (Bagged Logistic Regression)')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].legend(loc='lower right')\n",
    "\n",
    "# Learning Curve\n",
    "axes[1].plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n",
    "axes[1].plot(train_sizes, val_mean, 'o-', color='red', label='Validation Score')\n",
    "axes[1].set_title('Learning Curve (Bagged Logistic Regression)')\n",
    "axes[1].set_xlabel('Training Examples')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0842159-d961-4627-b984-8fe86f6ededa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# META-LEARNING MODELS\n",
    "# -----------------------------\n",
    "# -----------------------------\n",
    "## Super Learners (Stacking)\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea582e09-7cda-412d-ab2e-6f3c27ecffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Import Required Libraries\n",
    "# ----------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import (AdaBoostClassifier, GradientBoostingClassifier, \n",
    "                              RandomForestClassifier, HistGradientBoostingClassifier)\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ----------------------------\n",
    "# Load Dataset\n",
    "# ----------------------------\n",
    "file_path = \"heart_statlog_cleveland_hungary_final.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace spaces in column names with underscores\n",
    "df.columns = [col.replace(\" \", \"_\") for col in df.columns]\n",
    "print(df.head())\n",
    "\n",
    "# Features (X) and target variable (y)\n",
    "X = df.drop(columns=[\"target\"])\n",
    "y = df[\"target\"]\n",
    "\n",
    "# ----------------------------\n",
    "# Define Base Learners\n",
    "# ----------------------------\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "gbm = GradientBoostingClassifier(random_state=42)\n",
    "xgb = XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
    "lgb = LGBMClassifier(random_state=42, verbose=-1)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "s_gbm = GradientBoostingClassifier(subsample=0.8, random_state=42)  # Stochastic Gradient Boosting\n",
    "ada_rf = AdaBoostClassifier(estimator=RandomForestClassifier(random_state=42), \n",
    "                            random_state=42)\n",
    "\n",
    "# ----------------------------\n",
    "# Define Meta Learner\n",
    "# ----------------------------\n",
    "meta_learner = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# ----------------------------\n",
    "# Hyperparameter Grids\n",
    "# ----------------------------\n",
    "param_grids = {\n",
    "    'ada': {\n",
    "        'n_estimators': [50, 100, 150],       # Number of estimators\n",
    "        'learning_rate': [0.01, 0.1, 1],      # Learning rate\n",
    "        'algorithm': ['SAMME', 'SAMME.R']     # Algorithm type\n",
    "    },\n",
    "    'gbm': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    'xgb': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    'lgb': {\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    's_gbm': {\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.6, 0.8, 1.0]\n",
    "    },\n",
    "    'rf': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'ada_rf': {\n",
    "        'estimator__n_estimators': [50, 100, 150],  # Number of RF trees\n",
    "        'estimator__max_depth': [3, 5, 10],         # RF depth\n",
    "        'n_estimators': [50, 100, 150],                  # Number of AdaBoost trees\n",
    "        'learning_rate': [0.01, 0.1, 1]                  # Learning rate\n",
    "    }\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Run GridSearchCV for Each Base Learner\n",
    "# ----------------------------\n",
    "grid_search_results = {}\n",
    "for model_name, model in zip(\n",
    "    ['ada', 'gbm', 'xgb', 'lgb', 'rf', 's_gbm', 'ada_rf'],\n",
    "    [ada, gbm, xgb, lgb, rf, s_gbm, ada_rf]\n",
    "):\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model, \n",
    "        param_grid=param_grids[model_name], \n",
    "        cv=5, \n",
    "        n_jobs=-1, \n",
    "        verbose=0\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search_results[model_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Build Stacking Model with Optimized Base Learners\n",
    "# ----------------------------\n",
    "stacking_clf = StackingCVClassifier(\n",
    "    classifiers=[\n",
    "        grid_search_results['ada'], grid_search_results['gbm'], grid_search_results['xgb'], \n",
    "        grid_search_results['lgb'], grid_search_results['rf'], grid_search_results['s_gbm'], \n",
    "        grid_search_results['ada_rf']\n",
    "    ], \n",
    "    meta_classifier=meta_learner, \n",
    "    use_probas=True, \n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Train and Evaluate Stacking Model\n",
    "# ----------------------------\n",
    "stacking_clf.fit(X, y)\n",
    "stacking_accuracy = stacking_clf.score(X, y)\n",
    "print(f\"Accuracy of optimized stacking model: {stacking_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32fe8d-084a-43b6-93e2-bb44cd763b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# Model Evaluation and Visualization for Stacking Ensemble\n",
    "# =====================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split, learning_curve, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Train-Test Split\n",
    "# -----------------------------\n",
    "# Split dataset into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit stacking classifier on the training set\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# ROC Curve and AUC\n",
    "# -----------------------------\n",
    "# Predict probabilities for the positive class\n",
    "y_proba = stacking_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f'Stacking ROC (AUC = {roc_auc:.2f})', color='b')\n",
    "plt.plot([0,1], [0,1], 'k--')  # Diagonal line for random classifier\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Learning Curve\n",
    "# -----------------------------\n",
    "# Compute learning curves to assess bias-variance tradeoff\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    estimator=stacking_clf,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "# Compute mean accuracy scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Train Accuracy\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Validation Accuracy\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Confusion Matrix\n",
    "# -----------------------------\n",
    "# Predict classes on the test set\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75714035-883e-4458-987a-118a1fbf18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# Classification Report for Stacking Ensemble\n",
    "# =====================================================================\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# -----------------------------\n",
    "# Predictions on Test Set\n",
    "# -----------------------------\n",
    "y_pred_stacking = stacking_clf.predict(X_test)\n",
    "\n",
    "# -----------------------------\n",
    "# Classification Report\n",
    "# -----------------------------\n",
    "# Print detailed classification metrics (precision, recall, f1-score)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_stacking))\n",
    "\n",
    "# -----------------------------\n",
    "# Optional: Accuracy Score\n",
    "# -----------------------------\n",
    "accuracy = accuracy_score(y_test, y_pred_stacking)\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef1820-d683-4b37-9a9a-2a93e8be4b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# Define Optimized Base Learners\n",
    "# =====================================================================\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# -----------------------------\n",
    "# Base Learners with Optimized Hyperparameters\n",
    "# -----------------------------\n",
    "\n",
    "# AdaBoost classifier\n",
    "ada = AdaBoostClassifier(\n",
    "    algorithm='SAMME',\n",
    "    learning_rate=1,\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Gradient Boosting Machine (GBM)\n",
    "gbm = GradientBoostingClassifier(\n",
    "    learning_rate=0.2,\n",
    "    max_depth=5,\n",
    "    n_estimators=200,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# XGBoost classifier\n",
    "xgb = XGBClassifier(\n",
    "    learning_rate=0.2,\n",
    "    max_depth=5,\n",
    "    n_estimators=100,\n",
    "    subsample=1.0,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# LightGBM classifier\n",
    "lgb = LGBMClassifier(\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    n_estimators=200,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Random Forest classifier\n",
    "rf = RandomForestClassifier(\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    n_estimators=300,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Stochastic Gradient Boosting\n",
    "s_gbm = GradientBoostingClassifier(\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    n_estimators=100,\n",
    "    subsample=0.6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# AdaBoost with Random Forest as base estimator\n",
    "ada_rf = AdaBoostClassifier(\n",
    "    estimator=RandomForestClassifier(\n",
    "        max_depth=10,\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    ),\n",
    "    learning_rate=1,\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57917a-d3eb-44f0-8e3f-f42a29e96a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# Stacking Ensemble with Logistic Regression Meta Learner\n",
    "# =====================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare Features and Target\n",
    "# -----------------------------\n",
    "X = df.drop(columns=[\"target\"])  # Independent variables\n",
    "y = df[\"target\"]                 # Dependent variable\n",
    "\n",
    "# -----------------------------\n",
    "# Define Meta Learner (Logistic Regression with L2 regularization)\n",
    "# -----------------------------\n",
    "meta_learner = make_pipeline(\n",
    "    StandardScaler(),  # Standardize features\n",
    "    LogisticRegression(\n",
    "        penalty=\"l2\", \n",
    "        C=1.0, \n",
    "        solver=\"lbfgs\", \n",
    "        max_iter=500\n",
    "    )\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Define Stacking Classifier\n",
    "# -----------------------------\n",
    "stacking_lr = StackingCVClassifier(\n",
    "    classifiers=[ada, gbm, xgb, lgb, rf, s_gbm, ada_rf],  # Base models\n",
    "    meta_classifier=meta_learner,                        # Meta learner\n",
    "    use_probas=True,                                     # Use probabilistic predictions\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5-Fold Cross-Validation Accuracy\n",
    "# -----------------------------\n",
    "cv_scores = cross_val_score(stacking_lr, X, y, cv=5, scoring=\"accuracy\")\n",
    "print(f\"Stacking Model 5-Fold Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Fit Model on Full Dataset\n",
    "# -----------------------------\n",
    "stacking_lr.fit(X, y)\n",
    "stacking_accuracy = stacking_lr.score(X, y)\n",
    "print(f\"Stacking Model Accuracy on Full Data: {stacking_accuracy:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Train-Test Split Evaluation\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "stacking_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred_stacking = stacking_lr.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_stacking))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260d6bb9-a66e-4f13-a044-4c9dafc56d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# ROC Curve for Stacking Classifier\n",
    "# =====================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# -----------------------------\n",
    "# Predict Probabilities for the Positive Class\n",
    "# -----------------------------\n",
    "y_proba = stacking_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# -----------------------------\n",
    "# Compute ROC Curve and AUC\n",
    "# -----------------------------\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# -----------------------------\n",
    "# Plot ROC Curve\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})', color='b')\n",
    "plt.plot([0,1], [0,1], 'k--', label='Random Classifier')  # Diagonal line for random guessing\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1164e-acc8-4992-941b-b823e7955d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# Stacking Ensemble with XGBoost Meta Learner\n",
    "# =====================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare Features and Target\n",
    "# -----------------------------\n",
    "X = df.drop(columns=[\"target\"])  # Independent variables\n",
    "y = df[\"target\"]                 # Dependent variable\n",
    "\n",
    "# -----------------------------\n",
    "# Define Meta Learner (XGBoost)\n",
    "# -----------------------------\n",
    "meta_learner = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Define Stacking Classifier\n",
    "# -----------------------------\n",
    "stacking_xgb = StackingCVClassifier(\n",
    "    classifiers=[ada, gbm, xgb, lgb, rf, s_gbm, ada_rf],  # Base models\n",
    "    meta_classifier=meta_learner,                        # XGBoost as meta learner\n",
    "    use_probas=True,                                     # Use probabilistic predictions\n",
    "    cv=5,                                                # 5-Fold Cross Validation\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5-Fold Cross-Validation Accuracy\n",
    "# -----------------------------\n",
    "cv_scores = cross_val_score(stacking_xgb, X, y, cv=5, scoring=\"accuracy\")\n",
    "print(f\"Stacking Model 5-Fold Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Fit Model on Full Dataset\n",
    "# -----------------------------\n",
    "stacking_xgb.fit(X, y)\n",
    "stacking_accuracy = stacking_xgb.score(X, y)\n",
    "print(f\"Stacking Model Accuracy on Full Data: {stacking_accuracy:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Train-Test Split Evaluation\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "stacking_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred_stacking = stacking_xgb.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_stacking))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd6aa75-d2c4-4f97-9754-585429a9ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# ROC Curve for Stacking Classifier with XGBoost Meta Learner\n",
    "# =====================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# -----------------------------\n",
    "# Predict Probabilities for the Positive Class\n",
    "# -----------------------------\n",
    "y_proba = stacking_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# -----------------------------\n",
    "# Compute ROC Curve and AUC\n",
    "# -----------------------------\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# -----------------------------\n",
    "# Plot ROC Curve\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})', color='b')\n",
    "plt.plot([0,1], [0,1], 'k--', label='Random Classifier')  # Diagonal line for random guessing\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7079b72a-26d4-4da6-bbf5-d264bc2a8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# META-LEARNING MODELS\n",
    "# -----------------------------\n",
    "# -----------------------------\n",
    "## Light Optimization Blending\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b8df88-5b38-4612-89e9-1bc43433fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Import Required Libraries\n",
    "# ----------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ----------------------------\n",
    "# Load and Preprocess Dataset\n",
    "# ----------------------------\n",
    "dataset_path = \"heart_statlog_cleveland_hungary_final.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Replace spaces in column names with underscores for consistency\n",
    "df.columns = [col.replace(\" \", \"_\") for col in df.columns]\n",
    "print(df.head())\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[\"target\"]).values  # Convert to NumPy array\n",
    "y = df[\"target\"].values\n",
    "\n",
    "# ----------------------------\n",
    "# Train-Test Split\n",
    "# ----------------------------\n",
    "# 80% training, 20% testing with stratification to preserve class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Define Base Learners\n",
    "# ----------------------------\n",
    "# Ensemble of diverse classifiers\n",
    "base_models = [\n",
    "    (\"ada\", AdaBoostClassifier(n_estimators=100, random_state=42)),\n",
    "    (\"gbm\", GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    (\"xgb\", XGBClassifier(n_estimators=100, eval_metric=\"logloss\", random_state=42)),  # use_label_encoder removed\n",
    "    (\"lgb\", LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)),\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# Generate Base Model Predictions (5-Fold Blending)\n",
    "# ----------------------------\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "base_predictions_proba = np.zeros((X_train.shape[0], len(base_models)))\n",
    "test_predictions_proba = np.zeros((kf.get_n_splits(), X_test.shape[0], len(base_models)))\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    X_tr, X_val = X_train[train_index], X_train[val_index]\n",
    "    y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    for j, (name, model) in enumerate(base_models):\n",
    "        # Train base model on fold\n",
    "        model.fit(X_tr, y_tr)\n",
    "        \n",
    "        # Predict probabilities on validation fold\n",
    "        base_predictions_proba[val_index, j] = model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Predict probabilities on test set for current fold\n",
    "        test_predictions_proba[i, :, j] = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Average test set predictions across folds\n",
    "test_predictions_proba = test_predictions_proba.mean(axis=0)\n",
    "\n",
    "# ----------------------------\n",
    "# Train Meta-Model\n",
    "# ----------------------------\n",
    "# Logistic Regression is used as the meta-learner\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(base_predictions_proba, y_train)  # Fit on full training set\n",
    "meta_predictions_proba = meta_model.predict_proba(test_predictions_proba)[:, 1]\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate Model Performance\n",
    "# ----------------------------\n",
    "auc_score = roc_auc_score(y_test, meta_predictions_proba)\n",
    "print(f\"Blending Model AUC Score: {auc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd069df-5ab9-4944-9151-bff1fb079087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Import Required Libraries\n",
    "# ----------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ----------------------------\n",
    "# Load and Preprocess Dataset\n",
    "# ----------------------------\n",
    "dataset_path = \"heart_statlog_cleveland_hungary_final.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "df.columns = [col.replace(\" \", \"_\") for col in df.columns]  # Standardize column names\n",
    "print(df.head())\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[\"target\"]).values  # Convert features to NumPy array\n",
    "y = df[\"target\"].values\n",
    "\n",
    "# ----------------------------\n",
    "# Train-Test Split\n",
    "# ----------------------------\n",
    "# 80% training, 20% testing with stratification to preserve class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Define Base Learners\n",
    "# ----------------------------\n",
    "# Ensemble of classifiers for blending\n",
    "base_models = [\n",
    "    (\"ada\", AdaBoostClassifier(random_state=42)),\n",
    "    (\"gbm\", GradientBoostingClassifier(random_state=42)),\n",
    "    (\"xgb\", XGBClassifier(eval_metric=\"logloss\", random_state=42)),\n",
    "    (\"lgb\", LGBMClassifier(random_state=42, verbose=-1)),\n",
    "    (\"rf\", RandomForestClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# Define Hyperparameter Grids\n",
    "# ----------------------------\n",
    "# Parameter grids for each base learner for GridSearchCV\n",
    "param_grids = {\n",
    "    \"ada\": {\"n_estimators\": [50, 100, 150], \"learning_rate\": [0.01, 0.1, 1]},\n",
    "    \"gbm\": {\"n_estimators\": [50, 100, 150], \"learning_rate\": [0.01, 0.1, 0.2], \"max_depth\": [3, 5, 7]},\n",
    "    \"xgb\": {\"n_estimators\": [50, 100, 150], \"learning_rate\": [0.01, 0.1, 0.2], \"max_depth\": [3, 5, 7]},\n",
    "    \"lgb\": {\"n_estimators\": [50, 100, 150], \"learning_rate\": [0.01, 0.1, 0.2], \"max_depth\": [3, 5, 7]},\n",
    "    \"rf\": {\"n_estimators\": [50, 100, 150], \"max_depth\": [10, 20, None]}\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Generate Base Model Predictions (5-Fold Blending with Hyperparameter Tuning)\n",
    "# ----------------------------\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "base_predictions_proba = np.zeros((X_train.shape[0], len(base_models)))\n",
    "test_predictions_proba = np.zeros((kf.get_n_splits(), X_test.shape[0], len(base_models)))\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    X_tr, X_val = X_train[train_index], X_train[val_index]\n",
    "    y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    for j, (name, model) in enumerate(base_models):\n",
    "        print(f\"Tuning {name} model...\")\n",
    "        \n",
    "        # Apply GridSearchCV to find optimal hyperparameters\n",
    "        grid_search = GridSearchCV(model, param_grids[name], cv=3, scoring=\"roc_auc\", n_jobs=-1)\n",
    "        grid_search.fit(X_tr, y_tr)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_  # Retrieve best estimator\n",
    "        print(f\"Best Parameters for {name}: {grid_search.best_params_}\")\n",
    "        \n",
    "        # Predict probabilities on validation fold\n",
    "        base_predictions_proba[val_index, j] = best_model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Predict probabilities on test set for current fold\n",
    "        test_predictions_proba[i, :, j] = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Average test set predictions across folds\n",
    "test_predictions_proba = test_predictions_proba.mean(axis=0)\n",
    "\n",
    "# ----------------------------\n",
    "# Train Meta-Model\n",
    "# ----------------------------\n",
    "# Logistic Regression as the meta-learner\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(base_predictions_proba, y_train)  # Fit on entire training set\n",
    "meta_predictions_proba = meta_model.predict_proba(test_predictions_proba)[:, 1]\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate Model Performance\n",
    "# ----------------------------\n",
    "auc_score = roc_auc_score(y_test, meta_predictions_proba)\n",
    "print(f\"Blending Model AUC Score: {auc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e176c6ce-ac47-4abf-90f7-f1d4db164722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Import Required Libraries\n",
    "# ----------------------------\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    roc_curve, classification_report, confusion_matrix, precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Threshold Predictions\n",
    "# ----------------------------\n",
    "# Convert predicted probabilities to class labels using 0.5 threshold\n",
    "y_pred_labels = (meta_predictions_proba >= 0.5).astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# Basic Evaluation Metrics\n",
    "# ----------------------------\n",
    "auc_score = roc_auc_score(y_test, meta_predictions_proba)\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "precision = precision_score(y_test, y_pred_labels)\n",
    "recall = recall_score(y_test, y_pred_labels)\n",
    "f1 = f1_score(y_test, y_pred_labels)\n",
    "\n",
    "print(f\"Blending Model AUC Score: {auc_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Confusion Matrix\n",
    "# ----------------------------\n",
    "cm = confusion_matrix(y_test, y_pred_labels)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Class 0\", \"Class 1\"],\n",
    "            yticklabels=[\"Class 0\", \"Class 1\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# Classification Report\n",
    "# ----------------------------\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_labels, digits=4))\n",
    "\n",
    "# ----------------------------\n",
    "# ROC Curve and AUC\n",
    "# ----------------------------\n",
    "fpr, tpr, thresholds = roc_curve(y_test, meta_predictions_proba)\n",
    "auc = roc_auc_score(y_test, meta_predictions_proba)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# Precision-Recall Curve\n",
    "# ----------------------------\n",
    "precision_vals, recall_vals, thresholds_pr = precision_recall_curve(y_test, meta_predictions_proba)\n",
    "ap = average_precision_score(y_test, meta_predictions_proba)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(recall_vals, precision_vals, label=f\"AP = {ap:.4f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7ab677-0aed-4d2f-82a5-e99a9b252b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Learning Curve Visualization for Meta-Model\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, learning_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Define Cross-Validation Strategy\n",
    "# ----------------------------\n",
    "# Use StratifiedKFold to preserve class distribution\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----------------------------\n",
    "# Compute Learning Curve Data\n",
    "# ----------------------------\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    estimator=meta_model,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "meta_model = LogisticRegression(max_iter=2000, random_state=42)\n",
    "\n",
    "# Compute mean and standard deviation of train and test scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# ----------------------------\n",
    "# Plot Learning Curve\n",
    "# ----------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training Score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-Validation Score\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Number of Training Samples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c89932-9b84-4ca1-b099-c22d1c6b463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# META-LEARNING MODELS\n",
    "# -----------------------------\n",
    "# -----------------------------\n",
    "## Comprehensive Optimization Blending \n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5780e-58ec-4e1c-bdc9-c46abf74775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Import Required Libraries\n",
    "# ----------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ----------------------------\n",
    "# Load and Preprocess Dataset\n",
    "# ----------------------------\n",
    "dataset_path = \"heart_statlog_cleveland_hungary_final.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Standardize column names by replacing spaces with underscores\n",
    "df.columns = [col.replace(\" \", \"_\") for col in df.columns]\n",
    "print(df.head())\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[\"target\"]).values  # Convert features to NumPy array\n",
    "y = df[\"target\"].values\n",
    "\n",
    "# ----------------------------\n",
    "# Train-Test Split\n",
    "# ----------------------------\n",
    "# 80% training, 20% testing with stratification to preserve class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Define Base Learners\n",
    "# ----------------------------\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "gbm = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "xgb = XGBClassifier(n_estimators=100, eval_metric=\"logloss\", random_state=42)\n",
    "lgb = LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "s_gbm = GradientBoostingClassifier(n_estimators=100, subsample=0.8, random_state=42)  # Stochastic Gradient Boosting\n",
    "ada_rf = AdaBoostClassifier(\n",
    "    base_estimator=RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# List of base models for blending\n",
    "base_models = [\n",
    "    (\"ada\", ada),\n",
    "    (\"gbm\", gbm),\n",
    "    (\"xgb\", xgb),\n",
    "    (\"lgb\", lgb),\n",
    "    (\"rf\", rf),\n",
    "    (\"s_gbm\", s_gbm),\n",
    "    (\"ada_rf\", ada_rf)\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# Generate Base Model Predictions (5-Fold Blending)\n",
    "# ----------------------------\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "base_predictions_proba = np.zeros((X_train.shape[0], len(base_models)))\n",
    "test_predictions_proba = np.zeros((kf.get_n_splits(), X_test.shape[0], len(base_models)))\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    X_tr, X_val = X_train[train_index], X_train[val_index]\n",
    "    y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    for j, (name, model) in enumerate(base_models):\n",
    "        # Train base model on the fold\n",
    "        model.fit(X_tr, y_tr)\n",
    "        \n",
    "        # Predict probabilities on validation fold\n",
    "        base_predictions_proba[val_index, j] = model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Predict probabilities on test set for current fold\n",
    "        test_predictions_proba[i, :, j] = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Average test set predictions across folds\n",
    "test_predictions_proba = test_predictions_proba.mean(axis=0)\n",
    "\n",
    "# ----------------------------\n",
    "# Train Meta-Model\n",
    "# ----------------------------\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(base_predictions_proba, y_train)  # Fit on entire training set\n",
    "meta_predictions_proba = meta_model.predict_proba(test_predictions_proba)[:, 1]\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate Model Performance\n",
    "# ----------------------------\n",
    "auc_score = roc_auc_score(y_test, meta_predictions_proba)\n",
    "print(f\"Blending Model AUC Score: {auc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bda73e-aa96-4eda-8c66-9ce1b7573fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Import Required Libraries\n",
    "# ----------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warnings\n",
    "\n",
    "# ----------------------------\n",
    "# Load and Preprocess Dataset\n",
    "# ----------------------------\n",
    "dataset_path = \"heart_statlog_cleveland_hungary_final.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Standardize column names by replacing spaces with underscores\n",
    "df.columns = [col.replace(\" \", \"_\") for col in df.columns]\n",
    "print(df.head())\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[\"target\"]).values\n",
    "y = df[\"target\"].values\n",
    "\n",
    "# ----------------------------\n",
    "# Train-Test Split\n",
    "# ----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Define Base Learners\n",
    "# ----------------------------\n",
    "base_models = [\n",
    "    (\"ada\", AdaBoostClassifier(random_state=42)),\n",
    "    (\"gbm\", GradientBoostingClassifier(random_state=42)),\n",
    "    (\"xgb\", XGBClassifier(eval_metric=\"logloss\", random_state=42)),\n",
    "    (\"lgb\", LGBMClassifier(random_state=42, verbose=-1)),\n",
    "    (\"rf\", RandomForestClassifier(random_state=42)),\n",
    "    (\"s_gbm\", GradientBoostingClassifier(subsample=0.8, random_state=42)),\n",
    "    (\"ada_rf\", AdaBoostClassifier(estimator=RandomForestClassifier(random_state=42), random_state=42))\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# Define Hyperparameter Grids\n",
    "# ----------------------------\n",
    "param_grids = {\n",
    "    'ada': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1.0]},\n",
    "    'gbm': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.5]},\n",
    "    'xgb': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.5]},\n",
    "    'lgb': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.5]},\n",
    "    'rf': {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 10]},\n",
    "    's_gbm': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.5]},\n",
    "    'ada_rf': {'n_estimators': [50, 100, 200], 'estimator__n_estimators': [50, 100]}\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 5-Fold Cross-Validation for Blending\n",
    "# ----------------------------\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "base_predictions_proba = np.zeros((X_train.shape[0], len(base_models)))\n",
    "test_predictions_proba = np.zeros((kf.get_n_splits(), X_test.shape[0], len(base_models)))\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    X_tr, X_val = X_train[train_index], X_train[val_index]\n",
    "    y_tr, y_val = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    for j, (name, model) in enumerate(base_models):\n",
    "        print(f\"Tuning {name} model...\")\n",
    "        grid_search = GridSearchCV(model, param_grids[name], cv=3, scoring=\"roc_auc\", n_jobs=-1)\n",
    "        grid_search.fit(X_tr, y_tr)\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"Best Parameters for {name}: {grid_search.best_params_}\")\n",
    "\n",
    "        # Store validation probabilities\n",
    "        base_predictions_proba[val_index, j] = best_model.predict_proba(X_val)[:, 1]\n",
    "        # Store test probabilities for this fold\n",
    "        test_predictions_proba[i, :, j] = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Average test set predictions across folds\n",
    "test_predictions_proba = test_predictions_proba.mean(axis=0)\n",
    "\n",
    "# ----------------------------\n",
    "# Train Meta-Model\n",
    "# ----------------------------\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(base_predictions_proba, y_train)\n",
    "meta_predictions_proba = meta_model.predict_proba(test_predictions_proba)[:, 1]\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate Model Performance\n",
    "# ----------------------------\n",
    "auc_score = roc_auc_score(y_test, meta_predictions_proba)\n",
    "print(f\"\\nBlending Model AUC Score: {auc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2fe4a4-ce64-4a02-ab7e-934772bbf392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Performance Evaluation of Blending Meta-Model\n",
    "# ============================================================\n",
    "\n",
    "# ----------------------------\n",
    "# Import Required Libraries\n",
    "# ----------------------------\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "# Compute Evaluation Metrics\n",
    "# ----------------------------\n",
    "auc_score = roc_auc_score(y_test, meta_predictions_proba)\n",
    "accuracy = accuracy_score(y_test, meta_model.predict(test_predictions_proba))\n",
    "precision = precision_score(y_test, meta_model.predict(test_predictions_proba))\n",
    "recall = recall_score(y_test, meta_model.predict(test_predictions_proba))\n",
    "f1 = f1_score(y_test, meta_model.predict(test_predictions_proba))\n",
    "\n",
    "# Print performance metrics\n",
    "print(f\"Blending Model AUC Score: {auc_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Plot ROC Curve\n",
    "# ----------------------------\n",
    "fpr, tpr, thresholds = roc_curve(y_test, meta_predictions_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='b', label=f'ROC Curve (AUC = {auc_score:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d49ca-c986-441e-be95-ae3de42ada94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Detailed Classification Performance Evaluation\n",
    "# ============================================================\n",
    "\n",
    "# ----------------------------\n",
    "# Import Required Libraries\n",
    "# ----------------------------\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    roc_curve, \n",
    "    roc_auc_score, \n",
    "    precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Convert Probabilities to Binary Labels\n",
    "# ----------------------------\n",
    "y_pred_labels = (meta_predictions_proba >= 0.5).astype(int)  # Threshold = 0.5\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Confusion Matrix\n",
    "# ----------------------------\n",
    "cm = confusion_matrix(y_test, y_pred_labels)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "    xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"]\n",
    ")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Classification Report\n",
    "# ----------------------------\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_labels, digits=4))\n",
    "\n",
    "# ----------------------------\n",
    "# 3) ROC Curve + AUC\n",
    "# ----------------------------\n",
    "fpr, tpr, thresholds = roc_curve(y_test, meta_predictions_proba)\n",
    "auc = roc_auc_score(y_test, meta_predictions_proba)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, color='b', label=f\"AUC = {auc:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Precision-Recall Curve\n",
    "# ----------------------------\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_test, meta_predictions_proba)\n",
    "ap = average_precision_score(y_test, meta_predictions_proba)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(recall, precision, color='m', label=f\"Average Precision (AP) = {ap:.4f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0c6b7d-7e97-459a-859b-b8d009a45914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Import Required Libraries\n",
    "# ----------------------------\n",
    "from sklearn.model_selection import StratifiedKFold, learning_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Stratified K-Fold Cross-Validation for Learning Curve\n",
    "# ----------------------------\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Compute learning curve data\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    meta_model, X, y, cv=cv, scoring='accuracy', \n",
    "    n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10), random_state=42\n",
    ")\n",
    "\n",
    "meta_model = LogisticRegression(max_iter=3000, random_state=42)\n",
    "\n",
    "# Calculate mean and standard deviation of train/test scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# ----------------------------\n",
    "# Plot Learning Curve\n",
    "# ----------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Fill the areas for standard deviation\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "\n",
    "# Plot mean accuracy curves\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training Score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-Validation Score\")\n",
    "\n",
    "plt.title(\"Learning Curve of Meta-Model\")\n",
    "plt.xlabel(\"Number of Training Samples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4763ba1-e512-4554-8919-c3d366d3aeea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
